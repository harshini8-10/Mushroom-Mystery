{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f77c9d6",
   "metadata": {},
   "source": [
    "# MUSHROOM3a\n",
    "## 0.98440"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf2f8622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 136372, number of negative: 112983\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041010 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1270\n",
      "[LightGBM] [Info] Number of data points in the train set: 249355, number of used features: 129\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546899 -> initscore=0.188149\n",
      "[LightGBM] [Info] Start training from score 0.188149\n",
      "[LightGBM] [Info] Number of positive: 109098, number of negative: 90386\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028788 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1270\n",
      "[LightGBM] [Info] Number of data points in the train set: 199484, number of used features: 129\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546901 -> initscore=0.188157\n",
      "[LightGBM] [Info] Start training from score 0.188157\n",
      "[LightGBM] [Info] Number of positive: 109098, number of negative: 90386\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049858 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1270\n",
      "[LightGBM] [Info] Number of data points in the train set: 199484, number of used features: 129\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546901 -> initscore=0.188157\n",
      "[LightGBM] [Info] Start training from score 0.188157\n",
      "[LightGBM] [Info] Number of positive: 109098, number of negative: 90386\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030205 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1270\n",
      "[LightGBM] [Info] Number of data points in the train set: 199484, number of used features: 129\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546901 -> initscore=0.188157\n",
      "[LightGBM] [Info] Start training from score 0.188157\n",
      "[LightGBM] [Info] Number of positive: 109097, number of negative: 90387\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022095 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1270\n",
      "[LightGBM] [Info] Number of data points in the train set: 199484, number of used features: 129\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546896 -> initscore=0.188137\n",
      "[LightGBM] [Info] Start training from score 0.188137\n",
      "[LightGBM] [Info] Number of positive: 109097, number of negative: 90387\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023836 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1270\n",
      "[LightGBM] [Info] Number of data points in the train set: 199484, number of used features: 129\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546896 -> initscore=0.188137\n",
      "[LightGBM] [Info] Start training from score 0.188137\n",
      "Predictions saved to Mushroom3a.csv\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from packaging import version\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import PowerTransformer, RobustScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import KNNImputer\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "assert sys.version_info >= (3, 7)\n",
    "assert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\")\n",
    "\n",
    "# Custom transformer to shift values\n",
    "class ShiftTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, shift=1.0):\n",
    "        self.shift = shift\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X + self.shift\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        return X - self.shift\n",
    "\n",
    "# Load train.csv and apply sample size since the dataset is huge\n",
    "mushroom_df = pd.read_csv('train.csv').sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Removing Duplicates\n",
    "mushroom_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Modifying Column Names for Better Readability\n",
    "mushroom_df.columns = mushroom_df.columns.str.replace('-', '_')\n",
    "\n",
    "# Assign Feature variables and target variable\n",
    "X = mushroom_df.drop('class', axis=1)\n",
    "y = mushroom_df['class']\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Define attributes\n",
    "surrogate_attribs = [\"id\"]\n",
    "cat_attribs = ['cap_shape', 'cap_surface', 'cap_color', 'does_bruise_or_bleed', 'gill_attachment', 'gill_spacing', 'gill_color', 'stem_root', 'stem_surface', 'stem_color', 'veil_type', 'veil_color', 'has_ring', 'ring_type', 'spore_print_color', 'habitat', 'season']\n",
    "power_attribs = ['cap_diameter', 'stem_height', 'stem_width']\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown='ignore', sparse_output=True)),  # Keep sparse matrix output\n",
    "    (\"to_dense\", FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)),  # Convert to dense format\n",
    "    (\"imputer\", KNNImputer(n_neighbors=15)),  # Apply KNN imputation on the dense data\n",
    "])\n",
    "\n",
    "power_pipeline = Pipeline([\n",
    "    (\"imputer\", KNNImputer(n_neighbors=5)),\n",
    "    (\"shift_up\", ShiftTransformer(shift=abs(mushroom_df[power_attribs].min().min()) + 1)),\n",
    "    (\"log_transform\", PowerTransformer(method='box-cox')),\n",
    "    (\"shift_down\", ShiftTransformer(shift=-(abs(mushroom_df[power_attribs].min().min()) + 1))),\n",
    "    (\"scaler\", RobustScaler())\n",
    "])\n",
    "\n",
    "# Define the ColumnTransformer to apply different transformations to different columns\n",
    "preprocessing = ColumnTransformer([\n",
    "    (\"surrogate\", \"passthrough\", surrogate_attribs),\n",
    "    (\"categorical\", cat_pipeline, cat_attribs),\n",
    "    (\"power_transformer\", power_pipeline, power_attribs),\n",
    "])\n",
    "\n",
    "# Define models with updated hyperparameters\n",
    "catboost_model = CatBoostClassifier(depth=12, iterations=500, learning_rate=0.07, verbose=0)\n",
    "random_forest_model = RandomForestClassifier()\n",
    "lgbm_model = LGBMClassifier(num_leaves=31, n_estimators=500, learning_rate=0.05)\n",
    "\n",
    "# Split data into training and validation sets for proper fitting\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply preprocessing to training and validation data\n",
    "X_train_preprocessed = preprocessing.fit_transform(X_train)\n",
    "X_valid_preprocessed = preprocessing.transform(X_valid)\n",
    "\n",
    "# Load test data\n",
    "X_test = pd.read_csv('test.csv')\n",
    "\n",
    "# Modifying Column Names\n",
    "X_test.columns = X_test.columns.str.replace('-', '_')\n",
    "\n",
    "# Apply preprocessing to test data\n",
    "X_test_preprocessed = preprocessing.transform(X_test)\n",
    "\n",
    "\n",
    "# Define the stacking classifier\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('catboost', catboost_model),\n",
    "        ('random_forest', random_forest_model),\n",
    "        ('lgbm', lgbm_model)\n",
    "    ],\n",
    "    final_estimator=RandomForestClassifier(),\n",
    "    passthrough=True\n",
    ")\n",
    "\n",
    "# Fit the stacking classifier to the training data\n",
    "stacking_clf.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "\n",
    "# Directly apply the fitted stacking model to make predictions\n",
    "y_pred_stack_encoded = stacking_clf.predict(X_test_preprocessed)\n",
    "y_pred_stack = label_encoder.inverse_transform(y_pred_stack_encoded)\n",
    "\n",
    "# Save predictions to CSV file\n",
    "predictions_stack_df = pd.DataFrame({'id': X_test['id'], 'class': y_pred_stack})\n",
    "predictions_stack_df.to_csv('Mushroom3a.csv', index=False)\n",
    "\n",
    "print(\"Predictions saved to Mushroom3a.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d22f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
